# üï∑Ô∏è Gu√≠a Completa: Web Scraping para Noticias Tech

Esta gu√≠a te muestra c√≥mo scrapear diferentes fuentes de noticias tecnol√≥gicas sin necesidad de APIs.

---

## üéØ Fuentes que Puedes Scrapear

### ‚≠ê Nivel 1: F√ÅCIL (Ideal para empezar)

#### 1. **Blogs Oficiales de Empresas Tech**

Estas empresas publican en sus blogs oficiales que son f√°ciles de scrapear:

| Empresa | Blog URL | Frecuencia | Calidad |
|---------|----------|------------|---------|
| **Google AI** | `https://blog.google/technology/ai/` | Semanal | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê |
| **Microsoft** | `https://blogs.microsoft.com/` | Diaria | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê |
| **Meta AI** | `https://ai.meta.com/blog/` | Semanal | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê |
| **OpenAI** | `https://openai.com/news/` | Semanal | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê |
| **DeepMind** | `https://deepmind.google/discover/blog/` | Mensual | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê |
| **Anthropic** | `https://www.anthropic.com/news` | Mensual | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê |
| **Tesla** | `https://www.tesla.com/blog` | Mensual | ‚≠ê‚≠ê‚≠ê‚≠ê |
| **SpaceX** | `https://www.spacex.com/updates/` | Mensual | ‚≠ê‚≠ê‚≠ê‚≠ê |
| **GitHub** | `https://github.blog/` | Diaria | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê |
| **Stripe** | `https://stripe.com/blog` | Semanal | ‚≠ê‚≠ê‚≠ê‚≠ê |
| **Shopify** | `https://www.shopify.com/blog` | Diaria | ‚≠ê‚≠ê‚≠ê‚≠ê |
| **Notion** | `https://www.notion.so/blog` | Semanal | ‚≠ê‚≠ê‚≠ê‚≠ê |
| **Linear** | `https://linear.app/blog` | Mensual | ‚≠ê‚≠ê‚≠ê‚≠ê |
| **Vercel** | `https://vercel.com/blog` | Semanal | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê |
| **Netflix Tech** | `https://netflixtechblog.com/` | Mensual | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê |
| **Uber Engineering** | `https://www.uber.com/blog/engineering/` | Semanal | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê |
| **Airbnb Tech** | `https://medium.com/airbnb-engineering` | Mensual | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê |
| **Spotify Engineering** | `https://engineering.atspotify.com/` | Mensual | ‚≠ê‚≠ê‚≠ê‚≠ê |
| **AWS News** | `https://aws.amazon.com/blogs/aws/` | Diaria | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê |
| **Azure Updates** | `https://azure.microsoft.com/en-us/updates/` | Diaria | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê |

#### 2. **Hacker News** (S√∫per f√°cil de scrapear)

```
https://news.ycombinator.com/
```

- ‚úÖ HTML simple
- ‚úÖ No requiere JavaScript
- ‚úÖ API p√∫blica: https://github.com/HackerNews/API
- ‚úÖ Sin restricciones

#### 3. **Product Hunt**

```
https://www.producthunt.com/
```

- ‚úÖ Estructura HTML clara
- ‚úÖ Productos tech diarios
- ‚úÖ API disponible: https://api.producthunt.com/v2/docs

#### 4. **Indie Hackers**

```
https://www.indiehackers.com/
```

- ‚úÖ F√°cil de parsear
- ‚úÖ Historias de startups
- ‚úÖ HTML limpio

---

### ‚≠ê‚≠ê Nivel 2: INTERMEDIO (Requiere algo de experiencia)

#### 5. **Twitter/X** (Complicado pero posible)

**Opciones:**

**A) Nitter (Frontend alternativo - F√ÅCIL)**
```
https://nitter.net/username
```
- ‚úÖ Scraping f√°cil
- ‚úÖ No requiere API
- ‚úÖ Sin bloqueos
- ‚úÖ M√∫ltiples instancias disponibles

**Cuentas Tech para seguir:**
- `@elonmusk` - Tesla/SpaceX
- `@sama` - OpenAI
- `@satyanadella` - Microsoft
- `@sundarpichai` - Google
- `@getify` - JavaScript
- `@dan_abramov` - React
- `@paulg` - Y Combinator
- `@naval` - AngelList
- `@pmarca` - a16z
- `@balajis` - Crypto/Tech
- `@benedictevans` - Tech analyst
- `@cdixon` - a16z crypto
- `@dhh` - Ruby on Rails
- `@spolsky` - Stack Overflow

**B) Twitter API v2 (Requiere aprobaci√≥n)**
- ‚ö†Ô∏è Complicado de conseguir acceso
- ‚ö†Ô∏è Plan gratis muy limitado
- ‚ö†Ô∏è No recomendado

#### 6. **Reddit** (Sin API oficial)

**Opciones:**

**A) Old Reddit (F√°cil de scrapear)**
```
https://old.reddit.com/r/subreddit/.json
```
- ‚úÖ JSON p√∫blico
- ‚úÖ No requiere autenticaci√≥n
- ‚úÖ L√≠mites razonables

**Subreddits Tech relevantes:**
- `r/technology` - Tech general
- `r/programming` - Programaci√≥n
- `r/Python` - Python espec√≠fico
- `r/javascript` - JavaScript
- `r/webdev` - Web development
- `r/MachineLearning` - ML/AI
- `r/artificial` - AI news
- `r/datascience` - Data Science
- `r/devops` - DevOps
- `r/kubernetes` - Kubernetes
- `r/docker` - Docker
- `r/golang` - Go
- `r/rust` - Rust
- `r/reactjs` - React
- `r/nextjs` - Next.js
- `r/tailwindcss` - Tailwind
- `r/opensource` - Open Source
- `r/github` - GitHub
- `r/selfhosted` - Self-hosting
- `r/homelab` - Homelabs
- `r/sysadmin` - Sysadmin
- `r/netsec` - Security
- `r/hacking` - Ethical hacking
- `r/cryptography` - Crypto
- `r/blockchain` - Blockchain
- `r/cryptocurrency` - Crypto general
- `r/ethereum` - Ethereum
- `r/Bitcoin` - Bitcoin
- `r/startups` - Startups
- `r/entrepreneur` - Emprendimiento
- `r/SaaS` - SaaS products
- `r/indiehackers` - Indie devs
- `r/gamedev` - Game development
- `r/Unity3D` - Unity
- `r/unrealengine` - Unreal Engine

**B) Libreddit/Redlib (Frontend alternativo)**
```
https://libreddit.domain.glass/
```
- ‚úÖ F√°cil de scrapear
- ‚úÖ Sin JavaScript

#### 7. **Medium** (Publicaciones tech)

```
https://medium.com/tag/technology/latest
```

**Publicaciones relevantes:**
- `Better Programming`
- `The Startup`
- `JavaScript in Plain English`
- `Python in Plain English`
- `Level Up Coding`
- `Towards Data Science`
- `Analytics Vidhya`
- `UX Collective`

#### 8. **Dev.to**

```
https://dev.to/api/articles
```
- ‚úÖ API p√∫blica y gratuita
- ‚úÖ Sin autenticaci√≥n necesaria
- ‚úÖ Documentaci√≥n: https://developers.forem.com/api

---

### ‚≠ê‚≠ê‚≠ê Nivel 3: AVANZADO (Requiere herramientas especiales)

#### 9. **LinkedIn** (Complicado)

- ‚ö†Ô∏è Requiere login
- ‚ö†Ô∏è Anti-scraping agresivo
- ‚ö†Ô∏è Puede banear IPs
- üí° Alternativa: Buscar RSS feeds de blogs personales

#### 10. **Discord/Slack** (Comunidades tech)

- Requiere bots/webhooks
- Comunidades de empresas tech
- Anuncios de productos

---

## üõ†Ô∏è Herramientas de Scraping

### Python Libraries (ya tienes instalado)

```python
# HTML parsing
BeautifulSoup4   # Ya instalado
lxml             # Ya instalado

# HTTP requests
requests         # Ya instalado

# JavaScript rendering (si se necesita)
playwright       # Para sitios con JS
selenium         # Alternativa a Playwright

# APIs alternativas
tweepy           # Twitter (si consigues API)
praw             # Reddit (si consigues API)
```

---

## üìù Ejemplos de C√≥digo

### 1. Scraper para Blog de Empresa (Google AI)

```python
import requests
from bs4 import BeautifulSoup

def scrape_google_ai_blog():
    url = "https://blog.google/technology/ai/"
    response = requests.get(url)
    soup = BeautifulSoup(response.content, 'html.parser')
    
    articles = []
    # Encontrar los art√≠culos (inspecciona el HTML primero)
    for article in soup.find_all('article', class_='post'):
        title = article.find('h2').text.strip()
        link = article.find('a')['href']
        summary = article.find('p').text.strip()
        
        articles.append({
            'title': title,
            'link': link,
            'summary': summary,
            'source': 'Google AI Blog'
        })
    
    return articles
```

### 2. Scraper para Hacker News

```python
import requests

def scrape_hacker_news():
    # Usar la API oficial
    url = "https://hacker-news.firebaseio.com/v0/topstories.json"
    response = requests.get(url)
    story_ids = response.json()[:30]  # Top 30
    
    articles = []
    for story_id in story_ids:
        story_url = f"https://hacker-news.firebaseio.com/v0/item/{story_id}.json"
        story = requests.get(story_url).json()
        
        if story.get('type') == 'story':
            articles.append({
                'title': story.get('title'),
                'link': story.get('url'),
                'score': story.get('score'),
                'source': 'Hacker News'
            })
    
    return articles
```

### 3. Scraper para Reddit (sin API)

```python
import requests

def scrape_reddit_subreddit(subreddit='technology'):
    url = f"https://old.reddit.com/r/{subreddit}/.json"
    headers = {'User-Agent': 'Mozilla/5.0'}
    
    response = requests.get(url, headers=headers)
    data = response.json()
    
    posts = []
    for post in data['data']['children']:
        post_data = post['data']
        posts.append({
            'title': post_data['title'],
            'link': post_data['url'],
            'score': post_data['score'],
            'comments': post_data['num_comments'],
            'source': f'Reddit - r/{subreddit}'
        })
    
    return posts
```

### 4. Scraper para Nitter (Twitter alternativo)

```python
import requests
from bs4 import BeautifulSoup

def scrape_nitter_user(username='elonmusk'):
    url = f"https://nitter.net/{username}"
    headers = {'User-Agent': 'Mozilla/5.0'}
    
    response = requests.get(url, headers=headers)
    soup = BeautifulSoup(response.content, 'html.parser')
    
    tweets = []
    for tweet in soup.find_all('div', class_='timeline-item')[:10]:
        content = tweet.find('div', class_='tweet-content')
        if content:
            tweets.append({
                'text': content.text.strip(),
                'source': f'Twitter - @{username}',
                'author': username
            })
    
    return tweets
```

### 5. Scraper para Dev.to (con API)

```python
import requests

def scrape_dev_to(tag='python', per_page=20):
    url = f"https://dev.to/api/articles"
    params = {
        'tag': tag,
        'per_page': per_page
    }
    
    response = requests.get(url, params=params)
    articles = response.json()
    
    posts = []
    for article in articles:
        posts.append({
            'title': article['title'],
            'link': article['url'],
            'summary': article['description'],
            'tags': article['tag_list'],
            'source': 'Dev.to'
        })
    
    return posts
```

---

## üéØ Empresas Tech para Seguir

### FAANG + Big Tech

- **Google**: Blog AI, Cloud, Developer, Chrome
- **Meta/Facebook**: AI Blog, Engineering Blog
- **Amazon/AWS**: AWS News, Amazon Science
- **Netflix**: Tech Blog
- **Apple**: Newsroom (limitado)
- **Microsoft**: Azure, Developer Blogs
- **Nvidia**: AI Blog
- **Intel**: Newsroom

### Startups & Scale-ups

- **OpenAI**: News, Research
- **Anthropic**: News, Research
- **Stripe**: Blog, Developer Updates
- **Vercel**: Blog, Changelog
- **Supabase**: Blog
- **Railway**: Blog
- **Fly.io**: Blog
- **Render**: Blog
- **PlanetScale**: Blog
- **Neon**: Blog

### Developer Tools

- **GitHub**: Blog, Changelog
- **GitLab**: Blog
- **Docker**: Blog
- **Kubernetes**: Blog
- **Terraform**: Blog
- **Cloudflare**: Blog
- **Datadog**: Blog
- **Sentry**: Blog

### Languages & Frameworks

- **Python**: Blog oficial
- **JavaScript**: News
- **Rust**: Blog
- **Go**: Blog
- **React**: Blog
- **Vue**: Blog
- **Svelte**: Blog
- **Next.js**: Blog

---

## ‚ö†Ô∏è Consideraciones √âticas y Legales

### ‚úÖ BUENAS PR√ÅCTICAS

1. **Respetar robots.txt**
   ```python
   # Verificar antes de scrapear
   https://website.com/robots.txt
   ```

2. **Rate limiting**
   ```python
   import time
   time.sleep(1)  # 1 segundo entre requests
   ```

3. **User-Agent honesto**
   ```python
   headers = {
       'User-Agent': 'TechNewsBot/1.0 (contact@email.com)'
   }
   ```

4. **Cachear resultados**
   - No hacer requests innecesarios
   - Guardar datos localmente

### ‚ùå EVITAR

- ‚ùå Scrapear datos privados
- ‚ùå Hacer requests excesivos (DDoS)
- ‚ùå Ignorar Terms of Service
- ‚ùå Revender datos sin permiso

---

## üöÄ Recomendaciones Finales

### Empezar con:

1. **RSS Feeds** (33 ya configuradas) ‚Üê Ya tienes esto
2. **Hacker News API** (s√∫per f√°cil)
3. **Dev.to API** (gratis, sin l√≠mites)
4. **Reddit JSON** (sin autenticaci√≥n)
5. **Blogs de empresas** (Google, Microsoft, OpenAI)

### Despu√©s agregar:

6. **Nitter** para Twitter
7. **APIs pagadas** (Guardian, NewsData)
8. **Scraping avanzado** con Playwright

---

## üìö Pr√≥ximos Pasos

1. ¬øQuieres que cree scrapers para blogs de empresas espec√≠ficas?
2. ¬øImplemento scraper de Hacker News?
3. ¬øCreo scraper de Reddit sin API?
4. ¬øImplemento Nitter para seguir cuentas de Twitter?
5. ¬øDev.to API integration?

**¬øCu√°l te interesa m√°s?** üéØ
